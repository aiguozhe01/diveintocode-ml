{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 機械学習スクラッチ 決定木"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【考察】\n",
    "\n",
    "* どう決定木は作られていくか。\n",
    "* 以下の条件次第で、木の構成は変わる。\n",
    "    * 学習方法\n",
    "    * ハイパーパラメータ\n",
    "    * 訓練データ\n",
    "\n",
    "* 今回の決定木は量的変数のみに特化する。\n",
    "    * カテゴリ変数には「0と1」で代用する。\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDecesionTreeClassifierDepth1():\n",
    "    \"\"\"\n",
    "    深さ1の決定木分類器のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=False):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.verbose = verbose\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        決定木分類器を学習する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print()\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        決定木分類器を使いラベルを推定する\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】不純度を求める関数（CART式）\n",
    "\n",
    "### ノードのジニ不純度を計算する関数を作成してください\n",
    "\n",
    "* ジニ不純度とは、そのノードでのサンプルのクラスの異なりが同程度存在する確率。\n",
    "    * 確率が高いとノード内のサンプルが全て、異なるクラスに属している。\n",
    "        * データが半々なのは悪い分類\n",
    "    * 確率が低いとノード内のサンプルが全て、同じクラスに属している。\n",
    "    * ベルヌーイ分布における、全てのクラスの分散の和に相当する。\n",
    "* ノード内の不純度を最大限減らす（ジニ不純度が低い）素性と閾値の組を選ぶために、ジニ不純度を用いる。\n",
    "* 不純度が最も低ければジニ不純度の値は0、不純度が高くなればなるほどジニ不純度の値が1に漸近する。（[参照先url](https://qiita.com/3000manJPY/items/ef7495960f472ec14377)）\n",
    "* 最終的に情報利得Δgainで算出する。\n",
    "    * 利得が高い特徴と閾値ほど、不純度を最大限減らせる。\n",
    "    \n",
    "\n",
    "\n",
    "1. ジニ係数を算出する関数を構築する。\n",
    "2. ジニ係数を用い、情報利得を算出する関数を構築する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_coef(*args):\n",
    "    \"\"\"\"\"\n",
    "    ジニ係数を算出する。\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    *args : int\n",
    "        根ノード内の各特徴量毎のサンプル数（値）を渡す。\n",
    "        \n",
    "    return\n",
    "    ----------\n",
    "    ジニ係数\n",
    "    \"\"\"\"\"\n",
    "    sample_all = sum(args)\n",
    "    gini_coef_answer = 0\n",
    "    for i in range(len(args)):\n",
    "        gini_coef_answer += np.power(args[i]/sample_all, 2)\n",
    "    return 1 - gini_coef_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "例題1のジニ不純度: 0.5\n",
      "\n",
      "例題2のジニ不純度: 0.67\n",
      "\n",
      "例題3のジニ不純度: 0.48\n",
      "\n",
      "例題4のジニ不純度: 0.0\n"
     ]
    }
   ],
   "source": [
    "# クラス1:サンプル数15, クラス2:サンプル数15 → ジニ不純度0.500\n",
    "print(f'例題1のジニ不純度: {gini_coef(15, 15)}')\n",
    "print()\n",
    "# クラス1:サンプル数15, クラス2:サンプル数15, クラス3:サンプル数15 → ジニ不純度0.667\n",
    "print(f'例題2のジニ不純度: {gini_coef(15, 15, 15) :.2f}')\n",
    "print()\n",
    "# クラス1:サンプル数18, クラス2:サンプル数12 → ジニ不純度0.480\n",
    "print(f'例題3のジニ不純度: {gini_coef(18, 12)}')\n",
    "print()\n",
    "# クラス1:サンプル数30, クラス2:サンプル数0 → ジニ不純度0.000\n",
    "print(f'例題4のジニ不純度: {gini_coef(30, 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】情報利得を求める関数\n",
    "\n",
    "* 問題1で算出した確率はジニ不純度（ジニ係数）$I(t)$をroot_node $I(p)$として用いる。\n",
    "* 左右各ノードのサンプル数を引数として情報利得を算出する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(left_node_1, left_node_2, right_node_1, right_node_2):\n",
    "    \"\"\"\"\"\n",
    "    情報利得を算出する。\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    left_node_1 : int\n",
    "        左ノード内の第1特徴量のサンプル数（値）を渡す。\n",
    "    left_node_2 : int\n",
    "        左ノード内の第2特徴量のサンプル数（値）を渡す。\n",
    "    right_node_1 : int\n",
    "        右ノード内の第1特徴量のサンプル数（値）を渡す。\n",
    "    right_node_2 : int\n",
    "        右ノード内の第2特徴量のサンプル数（値）を渡す。\n",
    "        \n",
    "    return\n",
    "    ----------\n",
    "    gain : float\n",
    "        情報利得\n",
    "    \"\"\"\"\"    \n",
    "    sample_all_list = [left_node_1, left_node_2, right_node_1, right_node_2] # パラメータのリスト化\n",
    "    sample_all = sum(sample_all_list) # 全サンプルの総和\n",
    "    \n",
    "    left_all = np.add(left_node_1, left_node_2) # 左分岐の総和\n",
    "    # print(f'left_all: {left_all}')\n",
    "    right_all = np.add(right_node_1, right_node_2) # 右分岐の総和\n",
    "    # print(f'right_all: {right_all}')\n",
    "    root_node = [left_node_1 + right_node_1, left_node_2 + right_node_2] # 根ノードの左右総和数をリスト化\n",
    "    # print(f'root_node: {root_node}')\n",
    "    \n",
    "    gini_coef_answer = 0 \n",
    "    for i in range(len(root_node)):\n",
    "        gini_coef_answer += np.power(root_node[i]/sample_all, 2)\n",
    "    gini_coef = 1 - gini_coef_answer # ジニ係数\n",
    "    # print(f'gini_coef: {gini_coef}')\n",
    "    \n",
    "    left_node_coef = 1 - (np.power(left_node_1/left_all, 2) + np.power(left_node_2/left_all, 2)) # 左ノードのジニ係数\n",
    "    # print(f'left_node_coef: {left_node_coef}')\n",
    "    right_node_coef = 1 - (np.power(right_node_1/right_all, 2) + np.power(right_node_2/right_all, 2)) # 右ノードのジニ係数\n",
    "    # print(f'right_node_coef: {right_node_coef}')\n",
    "    gain = gini_coef - ((left_all/sample_all) * left_node_coef) - ((right_all/sample_all) * right_node_coef) # 情報利得の算出\n",
    "    \n",
    "    # print(f'左辺: {(left_all/sample_all) * left_node_coef}')\n",
    "    # print(f'右辺: {(right_all/sample_all) * right_node_coef}')\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "例題の情報利得：0.143\n",
      "スクラッチ関数の利得：0.143\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# 左ノードクラス1:サンプル数10, 左ノードクラス2:サンプル数30, 右ノードクラス1:サンプル数20, 右ノードクラス2:サンプル数5 → 情報利得0.143\n",
    "answer = info_gain(10, 30, 20, 5)\n",
    "print('例題の情報利得：0.143')\n",
    "print(f'スクラッチ関数の利得：{answer:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】学習\n",
    "\n",
    "1. 全サンプルの情報利得を算出する。\n",
    "    0. 説明変数Xと目的変数yを結合させる。\n",
    "    1. 根ノードの説明変数Xを、目的変数yを基に分別する。\n",
    "        * 目的変数0 : 左\n",
    "        * 説明変数1 : 右\n",
    "    2. 説明変数Xから、行列内のインデックスを一つ抽出する。\n",
    "    3. 抽出した引数を閾値として、葉ノードに振り分ける。\n",
    "        * 閾値以下：左\n",
    "        * 閾値以上：右\n",
    "    4. 左右の葉ノード内も目的変数を基に分別する。\n",
    "        * 目的変数0 : 左\n",
    "        * 説明変数1 : 右\n",
    "    5. 問題2の関数を用いて、情報利得を算出する。\n",
    "        * その関数に必要なパラメータである、葉ノード内の分別済説明変数（サンプル数）を引用する。\n",
    "    6. 算出した情報利得を新しい行列内に追記する。\n",
    "    7. ステップBに戻り、次のインデックスを抽出する。（繰り返す）\n",
    "    8. 全てのインデックスを基に算出した情報利得を代入した行列を完成させる。\n",
    "2. 最大値の情報利得を抽出する。\n",
    "    1. numpyの特性上、情報利得を代入した行列内の各列毎の最大値を抽出する。\n",
    "    2. 各列毎の最大値の中で、最も高い数値の要素とインデックスを抽出する。\n",
    "    3. 抽出した要素とインデックスをインスタンス化させて、次の推定に活用する。\n",
    "    \n",
    "    \n",
    "    \n",
    "    閾値をとりあえず設定する。\n",
    "    データを2領域に分割する\n",
    "    分割した領域の中でラベル事の個数を求める。\n",
    "    利得を求める。\n",
    "    以降、繰り返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "X = np.array([[1, 2],[3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "y = np.array([0, 1, 1, 0, 1]).reshape(5, 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    \"\"\"\n",
    "    決定木分類器を学習する\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データの特徴量\n",
    "    y : 次の形のndarray, shape (n_samples, )\n",
    "        訓練データの正解値\n",
    "    \"\"\"\n",
    "\n",
    "    root_node_array = np.append(X, y, axis=1) # 横(axis=1)に結合させる\n",
    "    \n",
    "    # 1_A\n",
    "    root_node_0 = root_node_array[:, np.all(root_node_array[:, 3] == 0, axis=0)]\n",
    "        \n",
    "    \n",
    "    root_node_0 = left_node_1 + right_node_1\n",
    "    root_node_1 = left_node_2 + right_node_2\n",
    "    \n",
    "    root_node = [root_node_0, root_node_1] # 根ノードの左右総和数をリスト化\n",
    "    # print(f'root_node: {root_node}')\n",
    "    \n",
    "    \n",
    "    if self.verbose:\n",
    "        #verboseをTrueにした際は学習過程を出力\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47337278106508873"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node = [40, 25]\n",
    "sample_all = sum(root_node)\n",
    "gini_coef_answer = 0 \n",
    "\n",
    "for i in range(len(root_node)):\n",
    "    gini_coef_answer += np.power(root_node[i]/sample_all, 2)\n",
    "gini_coef = 1 - gini_coef_answer # ジニ係数\n",
    "\n",
    "gini_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "X = np.array([[1, 2],[3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "y = np.array([0, 1, 1, 0, 1]).reshape(5, 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  0],\n",
       "       [ 3,  4,  1],\n",
       "       [ 5,  6,  1],\n",
       "       [ 7,  8,  0],\n",
       "       [ 9, 10,  1]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node_array = np.append(X, y, axis=1)\n",
    "root_node_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False,  True, False])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node_0 = np.any(root_node_array[:, 2:3] == 0, axis=1)\n",
    "root_node_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  0],\n",
       "       [ 3,  4,  1],\n",
       "       [ 5,  6,  1],\n",
       "       [ 7,  8,  0],\n",
       "       [ 9, 10,  1]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
