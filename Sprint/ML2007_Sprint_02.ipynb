{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint_2 機械学習スクラッチ入門\n",
    "\n",
    "-----\n",
    "**【目的】**\n",
    "* 機械学習手法のスクラッチ課題に取り組む準備を行う。scikit-learnを用いて分類・回帰問題を解くコードを書いておき、今後のSprintではそれと同じ動作をするクラスをスクラッチで作成する。\n",
    "* Numpyなどの基本的なライブラリを組み合わせて、scikit-learnのような応用的なライブラリと同じ機能のクラス・関数を自作する。\n",
    "* 上記を行い、scikit-learnなどのライブラリを動かすだけでは掴みづらい、アルゴリズムの深い理解を目指す。\n",
    "* また、今後の新たな手法に出会った時に理論・数式を理解しやすくする。\n",
    "* ライブラリを使う上での曖昧さを減らす。\n",
    "* 既存の実装を読みやすくする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】train_test_splitのスクラッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* スクラッチの練習として、scikit-learnのtrain_test_splitを自作する。以下の雛形をベースとして関数を完成させよ。\n",
    "* [sklearn.model_selection.train_test_split — scikit-learn 0.21.3 documentation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "* なお、作成した関数がscikit-learnのtrain_test_splitと同じ動作をしているか必ず確認をすること。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【確認】\n",
    "\n",
    "* train_test_split関数はscikit-learnライブラリ内にある一関数である。\n",
    "* データの前処理の一環として利用する。\n",
    "* 訓練データと検証データに指定した割合にランダムで分割する。\n",
    "* Xとyの配列を変数とし、Xとyの訓練データと検証データをそれぞれ出力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T02:35:50.038817Z",
     "start_time": "2020-11-10T02:35:50.033830Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def scratch_train_test_split(X, y, train_size=0.8):\n",
    "    \"\"\"\n",
    "    検証データを分割する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, )\n",
    "      正解値\n",
    "    train_size : float (0<train_size<1)\n",
    "      何割をtrainとするか指定\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    X_train : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    X_test : 次の形のndarray, shape (n_samples, n_features)\n",
    "      検証データ\n",
    "    y_train : 次の形のndarray, shape (n_samples, )\n",
    "      訓練データの正解値\n",
    "    y_test : 次の形のndarray, shape (n_samples, )\n",
    "      検証データの正解値\n",
    "    \n",
    "    Process\n",
    "    ----------\n",
    "    1. Randomizing\n",
    "        1. Using np.random.shuffle(), shuffle the two arrays.\n",
    "        2. Make pre_shuffle_x, pre_shuffle_y, post_shuffle_x, post_shuffle_y.\n",
    "\n",
    "    2. Dividing (Default: test_size 0.25, train_size 0.75)\n",
    "        1. Using this [tactics](https://stackoverflow.com/questions/58374049/split-a-list-with-a-adjustable-ratio) to split a list.\n",
    "        2. Make 2 splitted lists for train and test from each X and y\n",
    "    3. Return\n",
    "        1. Return value.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Intake parameter X, y and shuffle them.\n",
    "    np.random.shuffle(X)\n",
    "    np.random.shuffle(y)\n",
    "    \n",
    "    # Split X and y into given ratio using len() function.\n",
    "    elements_x = len(X)\n",
    "    \n",
    "    # print(f\"elements_x:{elements_x}\") # Return number of items\n",
    "    middle_x = int(elements_x * train_size) # The number of items is divided.\n",
    "    X_train, X_test = X[:middle_x], X[middle_x:] # \"middle_x\" becomes the mid point to seperate.\n",
    "    \n",
    "    # print(f\"x_train:{X_train}, x_test{X_test}\") # Sanity check.\n",
    "\n",
    "    elements_y = len(y) # Same logic as X.\n",
    "    middle_y = int(elements_y * train_size)\n",
    "    y_train, y_test = y[:middle_y], y[middle_y:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 【Scikit-learnとの検証】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T02:35:50.045798Z",
     "start_time": "2020-11-10T02:35:50.039814Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]]),\n",
       " [1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "X, y = np.arange(10).reshape((5, 2)), [1, 2, 3, 4, 5]\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learnの返り値 X_train_sklは\n",
      "[[4 5]\n",
      " [8 9]\n",
      " [0 1]\n",
      " [2 3]]\n",
      "scikit-learnの返り値 X_test_sklは[[6 7]]\n",
      "scikit-learnの返り値 y_train_sklは[4, 3, 1, 2]\n",
      "scikit-learnの返り値 y_testsklは[5]\n",
      "\n",
      "\n",
      "\n",
      "スクラッチの返り値 X_trainは\n",
      "[[4 5]\n",
      " [6 7]\n",
      " [2 3]\n",
      " [8 9]]\n",
      "スクラッチの返り値 X_testは[[0 1]]\n",
      "スクラッチの返り値 y_trainは[5, 1, 4, 3]\n",
      "スクラッチの返り値 y_testは[2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_skl, X_test_skl, y_train_skl, y_test_skl = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"scikit-learnの返り値 X_train_sklは\\n{}\".format(X_train))\n",
    "print(\"scikit-learnの返り値 X_test_sklは{}\".format(X_test))\n",
    "print(\"scikit-learnの返り値 y_train_sklは{}\".format(y_train))\n",
    "print(\"scikit-learnの返り値 y_testsklは{}\".format(y_test)) \n",
    "print()\n",
    "print()\n",
    "print()\n",
    "X_train, X_test, y_train, y_test = scratch_train_test_split(X, y)\n",
    "print(\"スクラッチの返り値 X_trainは\\n{}\".format(X_train))\n",
    "print(\"スクラッチの返り値 X_testは{}\".format(X_test))\n",
    "print(\"スクラッチの返り値 y_trainは{}\".format(y_train))\n",
    "print(\"スクラッチの返り値 y_testは{}\".format(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】 分類問題を解くコードの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-learnの３種類の分類手法を用いたコードを作成して、３種類のデータセットを分類検証させる。**\n",
    "\n",
    "-----\n",
    "\n",
    "1. ロジスティック回帰（SGDClassifier）を用い、3種類のデータセットを学習・推定するコードを作成する。\n",
    "2. SVMを用い、3種類のデータセットを学習・推定するコードを作成する。\n",
    "3. 決定木を用い、3種類のデータセットを学習・推定するコードを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【準備】3種類のデータセットを用意する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T09:10:36.109783Z",
     "start_time": "2020-11-11T09:10:33.542651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Irisデータセット\n",
    "# 二値分類とするため、virgicolorとvirginicaのみを目的変数、特徴量は4種全てを使う。\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# 基データからを特徴量（説明変数）をデータフレーム化。\n",
    "iris_raw_data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# 基データから特定の2品種(virgicolor:1, virginica:2)のみを抽出。\n",
    "# まず基データから目的変数を抽出したデータフレームを作成。\n",
    "iris_raw_species = pd.DataFrame(iris.target, columns=[\"species\"])\n",
    "# そのデータフレームから特定の2品種のキーのみを抽出。\n",
    "iris_species = iris_raw_species[iris_raw_species['species'].isin([1, 2])]\n",
    "\n",
    "# 上記2つのデータフレームを結合させる。\n",
    "iris_df = pd.concat([iris_raw_data, iris_species], join='inner', axis=1)\n",
    "\n",
    "# 説明変数と目的変数とでnumpy配列化\n",
    "X_iris = np.array(iris_df.iloc[:, :4])\n",
    "y_iris = np.array(iris_df.iloc[:, 4:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T02:53:50.117474Z",
     "start_time": "2020-11-10T02:53:50.077581Z"
    }
   },
   "outputs": [],
   "source": [
    "# シンプルデータセット１\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed=0)\n",
    "n_samples = 500\n",
    "f0 = [-1, 2]\n",
    "f1 = [2, -1]\n",
    "cov = [[1.0,0.8], [0.8, 1.0]]\n",
    "f0 = np.random.multivariate_normal(f0, cov, int(n_samples/2))\n",
    "f1 = np.random.multivariate_normal(f1, cov, int(n_samples/2))\n",
    "X = np.concatenate((f0, f1))\n",
    "y = np.concatenate((np.ones((int(n_samples/2))), np.ones((int(n_samples/2))) *(-1))).astype(np.int)\n",
    "random_index = np.random.permutation(np.arange(n_samples))\n",
    "X_simple1 = X[random_index]\n",
    "y_simple1 = y[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T02:54:52.665953Z",
     "start_time": "2020-11-10T02:54:52.658972Z"
    }
   },
   "outputs": [],
   "source": [
    "# シンプルデータセット２\n",
    "\n",
    "X_simple2 = np.array([[-0.44699 , -2.8073  ],[-1.4621  , -2.4586  ],\n",
    "       [ 0.10645 ,  1.9242  ],[-3.5944  , -4.0112  ],\n",
    "       [-0.9888  ,  4.5718  ],[-3.1625  , -3.9606  ],\n",
    "       [ 0.56421 ,  0.72888 ],[-0.60216 ,  8.4636  ],\n",
    "       [-0.61251 , -0.75345 ],[-0.73535 , -2.2718  ],\n",
    "       [-0.80647 , -2.2135  ],[ 0.86291 ,  2.3946  ],\n",
    "       [-3.1108  ,  0.15394 ],[-2.9362  ,  2.5462  ],\n",
    "       [-0.57242 , -2.9915  ],[ 1.4771  ,  3.4896  ],\n",
    "       [ 0.58619 ,  0.37158 ],[ 0.6017  ,  4.3439  ],\n",
    "       [-2.1086  ,  8.3428  ],[-4.1013  , -4.353   ],\n",
    "       [-1.9948  , -1.3927  ],[ 0.35084 , -0.031994],\n",
    "       [ 0.96765 ,  7.8929  ],[-1.281   , 15.6824  ],\n",
    "       [ 0.96765 , 10.083   ],[ 1.3763  ,  1.3347  ],\n",
    "       [-2.234   , -2.5323  ],[-2.9452  , -1.8219  ],\n",
    "       [ 0.14654 , -0.28733 ],[ 0.5461  ,  5.8245  ],\n",
    "       [-0.65259 ,  9.3444  ],[ 0.59912 ,  5.3524  ],\n",
    "       [ 0.50214 , -0.31818 ],[-3.0603  , -3.6461  ],\n",
    "       [-6.6797  ,  0.67661 ],[-2.353   , -0.72261 ],\n",
    "       [ 1.1319  ,  2.4023  ],[-0.12243 ,  9.0162  ],\n",
    "       [-2.5677  , 13.1779  ],[ 0.057313,  5.4681  ]])\n",
    "y_simple2 = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【解答2_1】ロジスティック回帰による学習・推定までのコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T03:31:24.451117Z",
     "start_time": "2020-11-10T03:31:24.446131Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(X, y, train_size=0.8):\n",
    "    \"\"\"\n",
    "    ロジスティック回帰による学習・推定まで行う。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, )\n",
    "      正解値\n",
    "    train_size : float (0<train_size<1)\n",
    "      何割をtrainとするか指定\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    predicted_test : ndarray, shape ()\n",
    "    　標準化済みの検証用データ（X_test_std）を用いての推定値\n",
    "    　以降の一致率検証用に用いる。\n",
    "    \n",
    "    Process\n",
    "    ----------\n",
    "    1. 交差検定を行うために、関数scratch_train_test_splitを用いてデータセットを分割する。\n",
    "    2. StandardScalerを用いて学習用データの特徴量を標準化する。\n",
    "    3. SGDClassifierを用いて学習する。\n",
    "    4. predictを用いて推定する。\n",
    "    \n",
    "    \"\"\"    \n",
    "    # scratch_train_test_split関数を用いてデータセットを分割する。\n",
    "    X_train, X_test, y_train, y_test = scratch_train_test_split(X, y, train_size)\n",
    "    \n",
    "    # 目的変数の1次元化\n",
    "    y_train = np.reshape(y_train,(-1))\n",
    "    y_test = np.reshape(y_test,(-1))\n",
    "    \n",
    "    # StandardScalerを用いてX_trainを標準化する。(pipelineを用いての省略も可)\n",
    "    # インスタンス化\n",
    "    sc = StandardScaler()\n",
    "    # 平均値と標準偏差値の学習\n",
    "    sc.fit(X_train)\n",
    "    \n",
    "    # X_trainとX_testを標準化\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "\n",
    "    # Process_3 SGDCLassifierを用いて学習する。\n",
    "    lr = SGDClassifier(loss='log')\n",
    "    lr.fit(X_train_std, y_train)\n",
    "    \n",
    "    # Process_4 fit後の推定(predict)を行う。\n",
    "    predicted_test = lr.predict(X_test_std)\n",
    "    \n",
    "    return predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T03:32:14.148159Z",
     "start_time": "2020-11-10T03:32:14.142175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Irisデータセットを用いての検証\n",
    "stochastic_gradient_descent(X_iris, y_iris, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T03:32:26.104173Z",
     "start_time": "2020-11-10T03:32:26.097192Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1,  1,  1,  1, -1, -1, -1,  1,  1,  1,  1,  1, -1,  1,  1,\n",
       "        1, -1, -1,  1, -1,  1, -1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,\n",
       "        1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1, -1,\n",
       "       -1,  1,  1, -1,  1, -1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1, -1,\n",
       "        1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,  1,\n",
       "        1, -1,  1,  1,  1,  1,  1, -1,  1,  1, -1, -1,  1, -1,  1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# シンプルデータセット1を用いての検証\n",
    "stochastic_gradient_descent(X_simple1, y_simple1, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T03:31:32.465675Z",
     "start_time": "2020-11-10T03:31:32.459692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# シンプルデータ2を用いての検証\n",
    "stochastic_gradient_descent(X_simple2, y_simple2, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 【解答2_2】SVMによる学習・推定までのコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T05:40:00.519681Z",
     "start_time": "2020-11-10T05:40:00.514695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def svm_function (X, y, train_size):\n",
    "    \"\"\"\n",
    "    SVMによる学習・推定まで行う。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, )\n",
    "      正解値\n",
    "    train_size : float (0<train_size<1)\n",
    "      何割をtrainとするか指定\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    predicted_test : ndarray, shape ()\n",
    "    　標準化済みの検証用データ（X_test_std）を用いての推定値\n",
    "    　以降の一致率検証用に用いる。\n",
    "    \n",
    "    Process\n",
    "    ----------\n",
    "    1. 交差検定を行うために、関数scratch_train_test_splitを用いてデータセットを分割する。\n",
    "    2. StandardScalerを用いて学習用データの特徴量を標準化する。\n",
    "    3. SVCを用いて学習する。\n",
    "    4. predictを用いて推定する。\n",
    "    \n",
    "    \"\"\"    \n",
    "    # scratch_train_test_split関数を用いてデータセットを分割する。\n",
    "    X_train, X_test, y_train, y_test = scratch_train_test_split(X, y, train_size)\n",
    "    \n",
    "    # 目的変数の1次元化\n",
    "    y_train = np.reshape(y_train,(-1))\n",
    "    y_test = np.reshape(y_test,(-1))\n",
    "    \n",
    "    # StandardScalerを用いてX_trainを標準化する。\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    # 平均値と標準偏差値の算出\n",
    "    sc.fit(X_train)\n",
    "    \n",
    "    # 標準化\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    \n",
    "    # SVCを用いて学習する。\n",
    "    model = SVC(gamma='scale')\n",
    "    model.fit(X_train_std, y_train)\n",
    "    \n",
    "    # fit後の推定(predict)を行う。\n",
    "    predicted_test = model.predict(X_test_std)\n",
    "    \n",
    "    return predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T05:40:00.833841Z",
     "start_time": "2020-11-10T05:40:00.827856Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Irisデータセットを用いての検証\n",
    "svm_function(X_iris, y_iris, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T05:33:28.466833Z",
     "start_time": "2020-11-10T05:33:28.456860Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1, -1,  1,  1,  1, -1,  1,  1,  1, -1,  1, -1,  1,  1, -1, -1,\n",
       "       -1,  1, -1,  1, -1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,\n",
       "        1, -1,  1,  1, -1,  1,  1, -1,  1,  1,  1,  1, -1, -1, -1,  1,  1,\n",
       "       -1, -1, -1,  1,  1,  1, -1, -1,  1,  1, -1,  1, -1, -1,  1, -1, -1,\n",
       "        1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,\n",
       "        1,  1,  1,  1, -1,  1,  1, -1,  1,  1, -1,  1, -1,  1,  1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# シンプルデータセット1を用いての検証\n",
    "svm_function(X_simple1, y_simple1, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T05:34:06.950905Z",
     "start_time": "2020-11-10T05:34:06.945918Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# シンプルデータ2を用いての検証\n",
    "svm_function(X_simple2, y_simple2, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 【解答2_3】決定木による学習・推定までのコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T05:40:19.996817Z",
     "start_time": "2020-11-10T05:40:19.992828Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "def tree_function (X, y, z):\n",
    "    \"\"\"\n",
    "    決定木による学習・推定まで行う。\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, )\n",
    "      正解値\n",
    "    z : 木の深さ\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    predicted_test : ndarray, shape ()\n",
    "    　標準化済みの検証用データ（X_test_std）を用いての推定値\n",
    "    　以降の一致率検証用に用いる。\n",
    "    \n",
    "    Process\n",
    "    ----------\n",
    "    1. tree.DecisionTreeClassifierを用いて学習する。\n",
    "    2. predictを用いて推定する。\n",
    "    \n",
    "    \"\"\"    \n",
    "    # scratch_train_test_split関数を用いてデータセットを分割する。\n",
    "    X_train, X_test, y_train, y_test = scratch_train_test_split(X, y)\n",
    "    \n",
    "    # 目的変数の1次元化\n",
    "    y_train = np.reshape(y_train,(-1))\n",
    "    y_test = np.reshape(y_test,(-1))\n",
    "    \n",
    "    # treeを用いて学習する。\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=z)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # fit後の推定(predict)を行う。\n",
    "    predicted_test = clf.predict(X_test)\n",
    "    \n",
    "    return predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T05:40:20.446639Z",
     "start_time": "2020-11-10T05:40:20.441651Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Irisデータセットを用いての検証\n",
    "tree_function(X_iris, y_iris, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T05:40:20.908404Z",
     "start_time": "2020-11-10T05:40:20.902419Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1,  1, -1,  1, -1,  1,  1, -1,  1,  1, -1,  1,  1, -1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,\n",
       "       -1,  1,  1, -1,  1,  1, -1,  1, -1,  1,  1,  1, -1,  1, -1,  1, -1,\n",
       "        1,  1,  1, -1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n",
       "       -1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1,  1, -1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1, -1,  1,  1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# シンプルデータセット1を用いての検証\n",
    "tree_function(X_simple1, y_simple1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T05:40:21.316311Z",
     "start_time": "2020-11-10T05:40:21.311326Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# シンプルデータセット2を用いての検証\n",
    "tree_function(X_simple2, y_simple2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 【問題3】 回帰問題を解くコードの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* House Pricesデータセットを学習・推定するコードをスクラッチする。\n",
    "    * House Pricesコンペティションのデータセットを利用する。\n",
    "        * train.csvをダウンロード。\n",
    "        * 目的変数：SalePrice\n",
    "        * 説明変数：GrLivArea, YearBuilt\n",
    "    * SGDRegressorクラスを利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T09:11:36.978934Z",
     "start_time": "2020-11-11T09:11:36.958988Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# データセットを用意する\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "house_df = pd.read_csv('../Data/house_prices_train.csv')\n",
    "\n",
    "# 目的変数\n",
    "y_house = np.array(house_df.loc[:, ['SalePrice']])\n",
    "\n",
    "# 説明変数\n",
    "X_house = np.array(house_df.loc[:, ['GrLivArea', 'YearBuilt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T06:09:35.905466Z",
     "start_time": "2020-11-10T06:09:35.900480Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def linear_regression(X, y, train_size=0.8):\n",
    "    \"\"\"\n",
    "    線形回帰による学習・推定まで行う。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, )\n",
    "      正解値\n",
    "    train_size : float (0<train_size<1)\n",
    "      何割をtrainとするか指定\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    predicted_test : ndarray, shape ()\n",
    "    　標準化済みの検証用データ（X_test_std）を用いての推定値\n",
    "    　以降の一致率検証用に用いる。\n",
    "    \n",
    "    Process\n",
    "    ----------\n",
    "    1. 交差検定を行うために、関数scratch_train_test_splitを用いてデータセットを分割する。\n",
    "    2. StandardScalerを用いて学習用データの特徴量を標準化する。\n",
    "    3. SGDClassifierを用いて学習する。\n",
    "    4. predictを用いて推定する。\n",
    "    \n",
    "    \"\"\"    \n",
    "    # scratch_train_test_split関数を用いてデータセットを分割する。\n",
    "    X_train, X_test, y_train, y_test = scratch_train_test_split(X, y, train_size)\n",
    "    \n",
    "    # 目的変数の1次元化\n",
    "    y_train = np.reshape(y_train,(-1))\n",
    "    y_test = np.reshape(y_test,(-1))\n",
    "    \n",
    "    # Process_2 StandardScalerを用いてX_trainを標準化する。\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    # 平均値と標準偏差値の算出\n",
    "    sc.fit(X_train)\n",
    "    \n",
    "    # 標準化\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    \n",
    "    # SGDCLassifierを用いて学習する。\n",
    "    reg = SGDRegressor(max_iter=1000)\n",
    "    reg.fit(X_train_std, y_train)\n",
    "    \n",
    "    # fit後の推定(predict)を行う。\n",
    "    predicted_test = reg.predict(X_test_std)\n",
    "    \n",
    "    return predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T06:09:36.814542Z",
     "start_time": "2020-11-10T06:09:36.801576Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175948.371611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200288.530674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171907.652631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>182356.798909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>184345.641940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>179252.671087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>184036.838078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>176513.714257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>185275.066471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>177908.925746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted_prices\n",
       "0       175948.371611\n",
       "1       200288.530674\n",
       "2       171907.652631\n",
       "3       182356.798909\n",
       "4       184345.641940\n",
       "..                ...\n",
       "287     179252.671087\n",
       "288     184036.838078\n",
       "289     176513.714257\n",
       "290     185275.066471\n",
       "291     177908.925746\n",
       "\n",
       "[292 rows x 1 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 推定結果\n",
    "predicted_test = linear_regression(X_house, y_house, 0.8)\n",
    "result = pd.DataFrame(predicted_test, columns=['predicted_prices'])\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "jupyter_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
