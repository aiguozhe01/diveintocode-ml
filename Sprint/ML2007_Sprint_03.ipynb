{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint_03 機械学習スクラッチ　線形回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T03:02:36.136598Z",
     "start_time": "2020-11-12T03:02:36.129617Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.coef_ = None\n",
    "        \n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        loss : numpy.float\n",
    "            訓練用データから算出した損失関数\n",
    "        val_loss : numpy.float\n",
    "            検証用データから算出した損失関数\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.coef_ = self._gradient_descent(X, y)\n",
    "        \n",
    "        self.loss = self._cost_function(hypothesis, y)\n",
    "        \n",
    "        self.val_loss = self._cost_function(X_pred, y_val)\n",
    "        \n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print()\n",
    "\n",
    "    def _gradient_descent(self, X, y):\n",
    "        \"\"\"\n",
    "        fitメソッドで呼び出された際に最急降下法による学習を行う。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self: class関数を引用する。\n",
    "\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "\n",
    "        error : \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          次の形のndarray, shape (n_samples, 1)\n",
    "          線形の仮定関数による推定結果    \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # h_thetaとyはnumpy.ndarrayであること\n",
    "        # inside_sigma = np.multiply((h_theta - y_train), X_train)\n",
    "\n",
    "        # ceof = coef - lr * (1/m) * np.sum(inside_sigma, axis=0)\n",
    "        # or\n",
    "        # coef -= (sum(_linear_hypothesis(X, coef)-y)/len(X)) * X\n",
    "\n",
    "        self.coef_ = self.coef_ - (self.lr/len(X)) * np.matmul((_linear_hypothesis(X, self.coef_)-y), X)\n",
    "\n",
    "        return self.coef_\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "\n",
    "        theta : 次の形のndarray, shape (n_samples, )\n",
    "         パラメータ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          次の形のndarray, shape (n_samples, 1)\n",
    "          線形の仮定関数による推定結果\n",
    "\n",
    "        Process\n",
    "        -------    \n",
    "          1. theta[0]はそのままにして、残りの行数分Xと配列数分y（ここではtheta）の積の和を求める。\n",
    "          2. Xとthetaの行列の積はnp.matmulを使う。\n",
    "          3. np.sumで積の総和を求める。\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        solution = self.coef_[0] + np.sum(np.matmul(self.coef_[1: ], X))\n",
    "        # solution = theta[0] + np.prod(theta[1: ], X)\n",
    "        # solution = np.dot(X, self.coef_.T)\n",
    "        return solution\n",
    "    \n",
    "    def _cost_function(y_pred, y):\n",
    "        \"\"\"\n",
    "        目的関数（損失関数）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : 次の形のndarray, shape (n_samples,)\n",
    "          推定した値\n",
    "        y : 次の形のndarray, shape (n_samples,)\n",
    "          正解値\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        loss : numpy.float\n",
    "          訓練用データと検証用データから算出した損失関数に用いられる。\n",
    "        \"\"\"\n",
    "\n",
    "        loss = np.square(np.subtract(y_pred, y)) / (2 * len(y))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        return _linear_hypothesis(self, X, self.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】仮定関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の数式で表される線形回帰の仮定関数を実装せよ。\n",
    "\n",
    "$\\begin{align}\n",
    "h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1+...+\\theta_jx_j+...+\\theta_nx_n (x_0=1)\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}(x_0=1)\\end{align}$のため、実際の式はこうなる。\n",
    "\n",
    "$\\begin{align}\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x_1+...+\\theta_jx_j+...+\\theta_nx_n (x_0=1)\n",
    "\\end{align}$\n",
    "### 【目的】\n",
    "\n",
    "* 上記の重回帰式を仮定する関数を作成する。\n",
    "\n",
    "### 【考察】\n",
    "\n",
    "* $\\begin{align}\\theta\\end{align}$ は直線の傾き（coefficients）を制御している。\n",
    "* $\\begin{align}x_0 = 1\\end{align}$ のため、切片は1である。\n",
    "* 行列 $\\begin{align}\\theta\\end{align}$ と行列 $\\begin{align}x_n\\end{align}$ の内積（np.dot or np.matmul）を算出する関数を作成する。\n",
    "* 内積を算出するため、引数xの行数分の配列数thetaをnp.randomを用いて算出する。\n",
    "\n",
    "### 【工程順序】\n",
    "1. np.random（0-10の範囲）を用いて、xの行数分の配列数thetaのarrayを構築\n",
    "2. x_train（ここでは引数x）を用いて$\\begin{align}h_\\theta\\end{align}$(x)を求める。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T01:42:00.860298Z",
     "start_time": "2020-11-12T01:42:00.355647Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T03:11:26.202476Z",
     "start_time": "2020-11-12T03:11:26.198488Z"
    }
   },
   "outputs": [],
   "source": [
    "def _linear_hypothesis(X, coef_):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    \n",
    "    theta : 次の形のndarray, shape (n_samples, )\n",
    "    　パラメータ\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果\n",
    "\n",
    "    Process\n",
    "    -------    \n",
    "      1. theta[0]はそのままにして、残りの行数分Xと配列数分y（ここではtheta）の積の和を求める。\n",
    "      2. Xとthetaの行列の積はnp.matmulを使う。\n",
    "      3. np.sumで積の総和を求める。\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # solution = coef_[0] + np.sum(np.matmul(coef_[1: ], X))\n",
    "    # solution = theta[0] + np.prod(theta[1: ], X)\n",
    "    print(coef_)\n",
    "    solution = np.dot(X, coef_.T)\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 【問題2】最急降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "最急降下法により学習させる実装を行なう。\n",
    "\n",
    "以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fitメソッドから呼び出すようにする。\n",
    "\n",
    "$\\begin{align}\n",
    "\\theta_j := \\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}[(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}]\n",
    "\\end{align}$\n",
    "\n",
    "ベクトル形式で表すとこうなる。\n",
    "\n",
    "$\\begin{align}\n",
    "\\theta := \\theta-\\alpha\\frac{1}{m}[(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}]\n",
    "\\end{align}$\n",
    "\n",
    "偏差を求める式がこの部分\n",
    "\n",
    "仮定した傾きから得た値yを実際のyとの差を求める部分である。\n",
    "\n",
    "$\\begin{align}\n",
    "(h_\\theta(x^{(i)})-y^{(i)})\n",
    "\\end{align}$\n",
    "\n",
    "-----\n",
    "\n",
    "$\\begin{align}:=\\end{align}$：左辺を右辺によって定義する。\n",
    "\n",
    "$\\begin{align}\\theta_j\\end{align}$：傾き \n",
    "```python \n",
    "_coef\n",
    "```\n",
    "$\\begin{align}\\alpha\\end{align}$：学習率\n",
    "```python \n",
    "self.lr\n",
    "```\n",
    "$\\begin{align}i\\end{align}$：サンプル（要素）のインデックス位置\n",
    "\n",
    "$\\begin{align}j\\end{align}$：特徴量のインデックス位置\n",
    "\n",
    "$\\begin{align}m\\end{align}$：サンプル（要素）の最大インデックス値（繰り返し回数の最大値）\n",
    "```python \n",
    "len(X)\n",
    "```\n",
    "$h_\\theta(x^{(i)})$：_linear_hypothesis\n",
    "\n",
    "$\\begin{align}y^{(i)}\\end{align}$：y_train\n",
    "\n",
    "$\\begin{align}x_j^{(i)}\\end{align}$：X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 【目的】\n",
    "\n",
    "* ひな形_gradient_descent（勾配降下）を用いて、最急降下法による機械学習が行えるように関数を完成させる。\n",
    "\n",
    "### 【考察】\n",
    "\n",
    "* 最急降下法とは、接線の傾き（$\\theta$）をゼロに近づくようにxの値を更新していき最適解に収束させる方法。\n",
    "* := の意味は「左辺を右辺で定義（代入）する」\n",
    "* 偏微分：特定の文字以外を定数とみなして微分したものを偏微分（偏導関数）と言います。\n",
    "    * 微分：ある関数の任意の点における傾き（$\\theta$）を導く式「導関数」を求めること。\n",
    "    * 傾き（$\\theta$）を求めるには2点間の変化の割合を求めること。\n",
    "        * $変化の割合=\\frac{yの増加量}{xの増加量}$\n",
    "\n",
    "最急降下法のアルゴリズム\n",
    "1. 対象とする関数を$(h_\\theta(x^{(i)})$とし、関数$(h_\\theta(x^{(i)})$の引数となるのがベクトルX\n",
    "2. $h_\\theta(x^{(i)})-y^{(i)}$が偏差となる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T01:42:00.871269Z",
     "start_time": "2020-11-12T01:42:00.866282Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _gradient_descent(self, X, y):\n",
    "    \"\"\"\n",
    "    fitメソッドで呼び出された際に最急降下法による学習を行う。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    self: class関数を引用する。\n",
    "    \n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "    \n",
    "    error : \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果    \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # h_thetaとyはnumpy.ndarrayであること\n",
    "    # inside_sigma = np.multiply((h_theta - y_train), X_train)\n",
    "    \n",
    "    # ceof = coef - lr * (1/m) * np.sum(inside_sigma, axis=0)\n",
    "    # or\n",
    "    # coef -= (sum(_linear_hypothesis(X, coef)-y)/len(X)) * X\n",
    "    \n",
    "    self.coef_ = self.coef_ - self.lr(1/len(X)) * np.matmul((_linear_hypothesis(X, coef_)-y), X)\n",
    "    \n",
    "    return self.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 【問題3】推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* 推定する仕組みを実装せよ。\n",
    "* ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えること。\n",
    "* 仮定関数 $h_\\theta(x)$ (_linear_hypothesis)の出力が推定結果とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T01:42:00.877252Z",
     "start_time": "2020-11-12T01:42:00.872266Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    \"\"\"\n",
    "    線形回帰を使い推定する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        サンプル\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形回帰による推定結果\n",
    "    \"\"\"\n",
    "    return _linear_hypothesis(self, X, _coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 【問題4】平均二乗誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* 線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成せよ。\n",
    "* 平均二乗誤差は以下の数式で表される。\n",
    "\n",
    "$\\begin{align}\n",
    "L(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2\n",
    "\\end{align}$\n",
    "\n",
    "$m$ : 入力されるデータの数 \n",
    "```python\n",
    "len(y)\n",
    "```\n",
    "$h_\\theta(x)$ : 仮定関数\n",
    "```pythonn\n",
    "y_pred\n",
    "```\n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "$y^{(i)}$ : i番目のサンプルの正解値\n",
    "```python\n",
    "y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T01:42:00.882239Z",
     "start_time": "2020-11-12T01:42:00.878250Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    \n",
    "    mse = np.square(np.subtract(y_pred, y)).mean()\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 【問題5】目的関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* 以下の数式で表される線形回帰の**目的関数（損失関数）**を実装せよ。\n",
    "* そして、これをself.loss, self.val_lossに記録すること。\n",
    "\n",
    "$\\begin{align}\n",
    "J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2\n",
    "\\end{align}$\n",
    "\n",
    "$m$ : 入力されるデータの数\n",
    "```python\n",
    "len(y)\n",
    "```\n",
    "$h_\\theta(X)$ : 仮定関数\n",
    "```python\n",
    "y_pred\n",
    "```\n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "$y^{(i)}$ : i番目のサンプルの正解値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T01:42:00.888223Z",
     "start_time": "2020-11-12T01:42:00.883236Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _cost_function(y_pred, y):\n",
    "    \"\"\"\n",
    "    目的関数（損失関数）\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    loss : numpy.float\n",
    "      訓練用データと検証用データから算出した損失関数に用いられる。\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = np.square(np.subtract(y_pred, y)) / (2 * len(y))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】学習と推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* House Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なうこと。\n",
    "* scikit-learnによる実装と比べ、正しく動いているかを確認せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T01:44:09.171018Z",
     "start_time": "2020-11-12T01:44:09.148081Z"
    }
   },
   "outputs": [],
   "source": [
    "# File system manangement\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "house_df = pd.read_csv('../Data/house_prices_train.csv')\n",
    "\n",
    "# 目的変数\n",
    "y_house = np.array(house_df.loc[:, ['SalePrice']])\n",
    "\n",
    "# 説明変数 GrLivArea, YearBuiltのみを抽出\n",
    "X_house = np.array(house_df.loc[:, ['GrLivArea', 'YearBuilt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T01:49:48.139155Z",
     "start_time": "2020-11-12T01:49:48.135165Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# 自作のtrain_test_splitを構築\n",
    "def scratch_train_test_split(X, y, train_size=0.8):\n",
    "    \"\"\"\n",
    "    検証データを分割する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, )\n",
    "      正解値\n",
    "    train_size : float (0<train_size<1)\n",
    "      何割をtrainとするか指定\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    X_train : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    X_test : 次の形のndarray, shape (n_samples, n_features)\n",
    "      検証データ\n",
    "    y_train : 次の形のndarray, shape (n_samples, )\n",
    "      訓練データの正解値\n",
    "    y_test : 次の形のndarray, shape (n_samples, )\n",
    "      検証データの正解値\n",
    "    \n",
    "    Process\n",
    "    ----------\n",
    "    1. Randomizing\n",
    "        1. Using np.random.shuffle(), shuffle the two arrays.\n",
    "        2. Make pre_shuffle_x, pre_shuffle_y, post_shuffle_x, post_shuffle_y.\n",
    "\n",
    "    2. Dividing (Default: test_size 0.25, train_size 0.75)\n",
    "        1. Using this [tactics](https://stackoverflow.com/questions/58374049/split-a-list-with-a-adjustable-ratio) to split a list.\n",
    "        2. Make 2 splitted lists for train and test from each X and y\n",
    "    3. Return\n",
    "        1. Return value.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Intake parameter X, y and shuffle them.\n",
    "    np.random.shuffle(X)\n",
    "    np.random.shuffle(y)\n",
    "    \n",
    "    # Split X and y into given ratio using len() function.\n",
    "    elements_x = len(X)\n",
    "    \n",
    "    # print(f\"elements_x:{elements_x}\") # Return number of items\n",
    "    middle_x = int(elements_x * train_size) # The number of items is divided.\n",
    "    X_train, X_test = X[:middle_x], X[middle_x:] # \"middle_x\" becomes the mid point to seperate.\n",
    "    \n",
    "    # print(f\"x_train:{X_train}, x_test{X_test}\") # Sanity check.\n",
    "\n",
    "    elements_y = len(y) # Same logic as X.\n",
    "    middle_y = int(elements_y * train_size)\n",
    "    y_train, y_test = y[:middle_y], y[middle_y:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T01:49:49.537414Z",
     "start_time": "2020-11-12T01:49:49.530432Z"
    }
   },
   "outputs": [],
   "source": [
    "# scratch_train_test_splitを実行\n",
    "\n",
    "X_train, X_test, y_train, y_test = scratch_train_test_split(X_house, y_house, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T02:26:59.900914Z",
     "start_time": "2020-11-12T02:26:58.978863Z"
    }
   },
   "outputs": [],
   "source": [
    "# 線形回帰のため、標準化を行う\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test) # X_testは未知のデータなので、fitするとX_train_stdにX_testの要素が混入するため、X_testはfitさせない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo\n",
    "1. ~~Scikit-learnの線形回帰を実装~~\n",
    "2. 自作の線形回帰を実装・回す。\n",
    "3. 上記の２モデルを比較検証\n",
    "4. アドバンス問題を挑戦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T02:35:50.978162Z",
     "start_time": "2020-11-12T02:35:50.973176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learnモデルのMSE：6.52e+09\n"
     ]
    }
   ],
   "source": [
    "# Scikit-learnの線形回帰を実装\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sk_lr = LinearRegression()\n",
    "\n",
    "sk_lr.fit(X_train_std, y_train)\n",
    "\n",
    "sk_pred = sk_lr.predict(X_test_std)\n",
    "\n",
    "sk_mse = mean_squared_error(y_test, sk_pred)\n",
    "\n",
    "print(f'scikit-learnモデルのMSE：{sk_mse:.3g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T03:11:29.599389Z",
     "start_time": "2020-11-12T03:11:29.583431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-2a2563913d1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mscratch_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScratchLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_bias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mscratch_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-53-ddbecc166c59>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cost_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-ddbecc166c59>\u001b[0m in \u001b[0;36m_gradient_descent\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# coef -= (sum(_linear_hypothesis(X, coef)-y)/len(X)) * X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_linear_hypothesis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-f8b9d042223d>\u001b[0m in \u001b[0;36m_linear_hypothesis\u001b[1;34m(X, coef_)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# solution = theta[0] + np.prod(theta[1: ], X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0msolution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msolution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "# 自作の線形回帰を実装\n",
    "scratch_lr = ScratchLinearRegression(num_iter=100, lr=0.1, no_bias=True, verbose=False)\n",
    "\n",
    "scratch_lr.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "jupyter_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
