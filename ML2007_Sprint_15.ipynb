{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML2007_Sprint_15.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7o2+jVnXwh/Wus14JzBg8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiguozhe01/diveintocode-ml/blob/master/ML2007_Sprint_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0yJAGNFmUQT",
        "colab_type": "text"
      },
      "source": [
        "# Sprint 論文読解入門\n",
        "\n",
        "* 参考文献：[[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99](https://arxiv.org/pdf/1506.01497.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB37XrqwnLCG",
        "colab_type": "text"
      },
      "source": [
        "# 【問題】\n",
        "\n",
        "それぞれについてJupyter Notebookにマークダウン形式で記述せよ。\n",
        "\n",
        "* 論文のどの部分から抜粋したかの記述\n",
        "* 必要に応じて先行研究（引用論文）を最低2つ引用する。\n",
        "* 論文の紹介記事を参照も可、ただし、根拠は論文内から。\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cViuIZVCnh9F",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## 1.   物体検出の分野にはどういった手法が存在したか。\n",
        "* 過去に存在した方法としては、SPPnet, R-CNN, Fast R-CNNである。\n",
        "  * SPPnet: 縦横比が異なるピラミッド型の領域提言を行うことによる、畳み込み型の画像検出方法。R-CNNより24-102倍の処理速度。[参照元](https://arxiv.org/abs/1406.4729)\n",
        "  * R-CNN: 従来の畳み込み式の画像学習に領域提言層を追加した方式。[参照元](https://arxiv.org/abs/1311.2524)\n",
        "  * Fast R-CNN: 従来は一枚の画像から各領域提言毎に畳み込み処理を行っていたが、このケースからは一度に全ての畳み込み処理を行うため、保存領域の確保から処理速度まで各段に短縮された。[参照元](https://arxiv.org/abs/1504.08083)\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7waEsHJPrELb",
        "colab_type": "text"
      },
      "source": [
        "## 2.   Fasterとあるが、どういった仕組みで高速化したのか。\n",
        "\n",
        "1.  従来のR-CNNと違い、Fast R-CNNは事前に作成した領域提言からではなく、最初に画像そのものを畳み込みニューラルネットワークに通して、特徴マップを作成する。\n",
        "  * 各画像1枚毎に1回のCNN処理で済む。\n",
        "  * CNNを通した後、領域提言を行う。\n",
        "\n",
        "2.  Faster R-CNNはFast R-CNNと違い、領域提言を行う際にSelective Search（SS）を用いず、Region Proposal Network（RPN）を用いて、検出アルゴリズムの代用としている。\n",
        "\n",
        "3.  領域提言（proposal）と検出時（detection）とで同じ畳み込み後の特徴量マップを共有している。\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA5CCDalvY-_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# 3.   One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
        "\n",
        "* OverFeat:\n",
        "  * one-stage\n",
        "  * class-specific検出法（パイプライン型）\n",
        "  * 範囲特徴量の抽出方法: 1ピラミッド上に1縦横比のwindowをスライドさせる。\n",
        "\n",
        "* RPN: \n",
        "  * two-stage cascade（縦繋ぎ）\n",
        "  * class-agnostic（レスポンシブ） proposals又はclass-specific検出法\n",
        "  * 範囲特徴量の抽出方法（1段階目）： \n",
        "    * RPN層（'attention'の役割）\n",
        "    * 3×3のwindowをスライドさせる。\n",
        "    * アンカーの作成と縦横比と縮尺毎で範囲抽出を行い、領域提案する。\n",
        "  * 範囲特徴量の抽出方法（2段階目）：\n",
        "    * RoI pooling層\n",
        "    * Fast R-CNNでも用いられる検出方法がこの段階で行われる\n",
        "      * objectの識別\n",
        "      * objectの位置の回帰\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sm83_adyxlz",
        "colab_type": "text"
      },
      "source": [
        "# 4.   RPNとは何か。\n",
        "\n",
        "* 入力画像を畳み込みニューラルネットワークに通して、出力したfeature mapを複数の縦横比と縮尺で領域提言したアンカーを複数作成して、RoI pooling層に引き渡すための層。\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVRTFtxsy7g_",
        "colab_type": "text"
      },
      "source": [
        "# 5.   RoIプーリングとは何か\n",
        "* RPN層から引き渡された複数のアンカーからobjectの識別と位置の回帰を行う。\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADa5VZDvy_3j",
        "colab_type": "text"
      },
      "source": [
        "# 6.   Anchorのサイズはどうするのが適切か。\n",
        "* feature map（畳み込み後の特徴マップ）の縦横の数値に、デフォルトで3種類の縦横比、3種類の縮尺数、を乗算した数が最適なAnchor数となる。\n",
        "\n",
        "<default値>\n",
        "* W = 入力画像の幅\n",
        "* H = 入力画像の高さ\n",
        "* k = 3種類の縦横比と3種類の縮尺数\n",
        "\n",
        "W × H × k = アンカー数\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXqZAmiRzCgb",
        "colab_type": "text"
      },
      "source": [
        "# 7.   何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
        "使用データセット\n",
        "* MS COCO object detection dataset\n",
        "  * 80 object categories\n",
        "  * 訓練用画像データ数：80k\n",
        "  * 認証用画像データ数：40k\n",
        "  * 検証用画像データ数：20k\n",
        "\n",
        "先行研究との比較\n",
        "- Fast R-CNN (SS, 2000, COCO train, COCO test-dev)\n",
        "  - mAP@.5: **39,3%**\n",
        "\n",
        "- Faster R-CNN (RPN, 300, COCO train, COCO test-dev)\n",
        "  - mAP@.5: **42.1%**\n",
        "\n",
        "同一の訓練データと検証データを用いて、凡そ3%の精度向上を算出している。\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpqDqll3zEER",
        "colab_type": "text"
      },
      "source": [
        "# 8.   （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。\n",
        "\n",
        "Single Shot Detector (SSD)\n",
        "\n",
        "* Faster R-CNNと類似機能\n",
        "  * 縦横比・縮尺がバラバラのアンカーボックスを用いている。\n",
        "  * アンカーボックスそのものではなく、オフセットを学習している。\n",
        "\n",
        "【参照元】\n",
        "* [The Battle of Speed vs. Accuracy: Single-Shot vs Two-Shot Detection Meta-Architecture](https://allegro.ai/blog/the-battle-of-speed-accuracy-single-shot-vs-two-shot-detection/)\n",
        "* [Zero to Hero: Guide to Object Detection using Deep Learning: Faster R-CNN,YOLO,SSD by ANKIT SACHAN](https://cv-tricks.com/object-detection/faster-r-cnn-yolo-ssd/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk0pcB5HDhNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wSWBcVUs9eb",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "----\n",
        "# Outline\n",
        "\n",
        "* SPPnet and Fast R-CNN have reduced the running times of the region proposal algorithms to hypothesize object locations.\n",
        "* The Faster R-CNN or the RPN is lot faster than the precedent.\n",
        "* The \"Region proposal step\" is the bottleneck in state-of-the-art detection systems.\n",
        "* region-based CNNs take advantage of GPUs, while the region proposal methods used in research are implemented on the CPU.\n",
        "  * Therefore, re-implement \"region proposal methods\" in GPU is the one-way.\n",
        "  * However, re-implementation ignores the down-stream detection network.\n",
        "* Region Proposal Networks (RPNs)\n",
        "  * The marginal cost for computing proposals is small\n",
        "    * (e.g., 10ms per image)\n",
        "* Fast R-CNN, the region-based detectors using conv-feature maps, can also be used for generating region proposals. \n",
        "\n",
        "----\n",
        "\n",
        "* RPN is constructed by adding a few additional convolutional layers.\n",
        "  * Layers that simultaneously regress region bounds and objectness scores at each location on a regular grid.\n",
        "  * RPN is like a fully convolutional network (FCN)\n",
        "    * FCN: Fully Convolutional Network\n",
        "    * It can be trained end-to-end specifically.\n",
        "  * Wide range of scales and aspect rations are predicted.\n",
        "  * The above way is done using *pyramid method*.\n",
        "  * A pyramid of regression references.\n",
        "    * Avoid enumerating images or filters of multiple scales.\n",
        "    * Performs well when trained and tested using single-scale images\n",
        "    * Shorter runnin speed.\n",
        "\n",
        "----\n",
        "# 1. INTRODUCTION\n",
        "\n",
        "* Unifying RPNs and Fast R-CNN\n",
        "  * \"Training scheme that alternates between fine-tuning for the region proposal task and then fine-tuning for object detection, while keeping the proposals fixed.\" \n",
        "\n",
        "* PASCAL VOC detection benchmarks are used.\n",
        "  * RPNs + Fast R-CNNs: Better score\n",
        "  * Selective Search + Fast R-CNNs: not as good\n",
        "\n",
        "* The new method (RPNs + Fast R-CNNs)\n",
        "  * Selective Search is waved.\n",
        "  * Proposal running time: 10ms\n",
        "  * VGG-16 model detection time: 5fps (in GPU)\n",
        "  * MS COCO data set gave good score.\n",
        "  * ILSVRC and COCO 2015: The basis of several 1st-place entries.\n",
        "\n",
        "* A preliminary version of RPN+Fast_R-CNN have been adopted to...\n",
        "  * 3D object detection\n",
        "  * Part-based detection\n",
        "  * Instance segmentation\n",
        "  * Image captioning\n",
        "  * Ceommercial: Pinterests (with user engagement improvement)\n",
        "\n",
        "* RPNs completely learn to propose regions from data, and thus can easily benefit from deeper and more expressive features.\n",
        "\n",
        "----\n",
        "# 2. RELATED WORK\n",
        "\n",
        "* **Object Proposals**\n",
        "  * Selective Search\n",
        "\n",
        "* **Deep Networks for Object Detection**\n",
        "  * R-CNN: \n",
        "    * Trains end-to-end to classify the proposal regions into object categories or background. \n",
        "    * Mainly plays as a classifier.\n",
        "  * MultiBox proposal network: single image crop or multiple large image crops.\n",
        "    * It does not share features between the proposal and detection networks.\n",
        "  * OverFeat: Computes convolutional features from an image pyramid for...\n",
        "    * classification\n",
        "    * localization\n",
        "    * detection\n",
        "  * DeepMask: For learning segmentation proposals.\n",
        "  * Adaptively-sized pooling (SPP): \n",
        "    * For efficient region-based object detection.\n",
        "    * For semantic segmentation.\n",
        "\n",
        "* **Faster R-CNN**\n",
        "  * Object detection system\n",
        "  * Composed in 2 modules.\n",
        "    1. RPN: Deep fully convolutional network that proposes regions.\n",
        "    2. Fast R-CNN detector that uses the proposed regions.\n",
        "  * Single unified network.\n",
        "  * Using 'Attention' mechanisms, recently popular terminology of neural networks. [31]\n",
        "\n",
        "----\n",
        "# 3. FASTER R-CNN\n",
        "\n",
        "## 3.1 Region Proposal Networks (RPN)\n",
        "\n",
        "* The Designs and Properties of the network for region proposal.\n",
        "\n",
        "\n",
        "## 3.2 Algrorithms for training 2 modules with features shared\n",
        "\n",
        "\n",
        "\n",
        "----\n",
        "# 4. Experiments\n",
        "* PASCAL VOC\n",
        "  * PASCAL VOC 2007\n",
        "    * 5k trainval images\n",
        "    * 5k test images\n",
        "    * 20 object categories\n",
        "  * PASCAL VOC 2012\n",
        "  * ImageNet pre-trained network\n",
        "    * Use \"fast\" version of ZF net [32]\n",
        "      * 5 convolutional layers\n",
        "      * 3 fully-connected layers\n",
        "  * VGG-16 model\n",
        "    * 13 convolutional layers\n",
        "    * 3 fully-connected layers\n",
        "  * mean Average Precision (mAP)\n",
        "    * Actual metric for object detection\n",
        "  * Fast R-CNN test results (ZF net)\n",
        "    * Selective Search (SS)\n",
        "      * 2000 proposals\n",
        "      * \"fast\" mode\n",
        "      * **mAP: 58.7%**\n",
        "    * EdgeBoxes (EB)\n",
        "      * 0.7 IoU (Intersection over Union)\n",
        "      * \"default\" setting\n",
        "      * **mAP: 58.6%**\n",
        "    * RPN + Fast R-CNN\n",
        "      * 300 proposals\n",
        "        * fewer proposals = faster and cheaper\n",
        "      * **mAP: 59.9%\n",
        "  * Ablation Experiments:\n",
        "    * The effect of sharing convolutional layers between the RPN and Fast R-CNN\n",
        "      * Stop after the 2/4 steps\n",
        "      * Train Fast R-CNN with 2000 SS proposals + ZF net\n",
        "        * (RPN doesn't share features with the detector)\n",
        "        * 300 SS proposals\n",
        "        * 100 SS proposals\n",
        "\n",
        "\n",
        "* MS COCO\n",
        "\n",
        "* MS COCO to PASCAL VOC\n",
        "\n",
        "\n",
        "\n",
        "----\n",
        "# 5. Conclusion\n",
        "* RPNs:\n",
        "  * Efficient and accurate proposal generation.\n",
        "  * Share convolutional features with the down-stream detection network.\n",
        "    * Makes it faster!\n",
        "  * The region proposal step is nearly cost-free!\n",
        "  * A unified, deep-learning-based object detection system to run at near real-time frame rates.\n",
        "  * The learned RPN also improves region proposal quality and thus the overall object detection accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viGLmdKo1ysz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}